{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nutritional-accountability",
   "metadata": {},
   "source": [
    "# DSCI: 525 Milestone 1 - Group 8\n",
    "\n",
    "## Rachel Wong, Rui Wang, Daniel Ortiz, Santiago Rugeles Schoonewolff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-grade",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprising-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# pyarrow and feather\n",
    "import pyarrow.feather as feather\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "muslim-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-laptop",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "floppy-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshare/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "still-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-projection",
   "metadata": {},
   "source": [
    "### Unzipping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informal-monte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 s, sys: 2.37 s, total: 5.13 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compressed-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 s, sys: 3.86 s, total: 23.2 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-broadcasting",
   "metadata": {},
   "source": [
    "### Combining data CSVs using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sound-stone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932835</th>\n",
       "      <td>2014-12-27 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.951144e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932836</th>\n",
       "      <td>2014-12-28 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.257118e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932837</th>\n",
       "      <td>2014-12-29 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>1.204670e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932838</th>\n",
       "      <td>2014-12-30 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.632404e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932839</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>3.431610e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932840 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0        1889-01-01 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "1        1889-01-02 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "2        1889-01-03 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "3        1889-01-04 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "4        1889-01-05 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "...                      ...      ...      ...      ...      ...   \n",
       "1932835  2014-12-27 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932836  2014-12-28 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932837  2014-12-29 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932838  2014-12-30 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932839  2014-12-31 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "\n",
       "         rain (mm/day)  \n",
       "0         3.293256e-13  \n",
       "1         0.000000e+00  \n",
       "2         0.000000e+00  \n",
       "3         0.000000e+00  \n",
       "4         1.047658e-02  \n",
       "...                ...  \n",
       "1932835   2.951144e-02  \n",
       "1932836   2.257118e-01  \n",
       "1932837   1.204670e-01  \n",
       "1932838   2.632404e-02  \n",
       "1932839   3.431610e-02  \n",
       "\n",
       "[1932840 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./figshare/ACCESS-CM2_daily_rainfall_NSW.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dress-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 404.39 MiB, increment: 0.05 MiB\n",
      "CPU times: user 5min 29s, sys: 19.5 s, total: 5min 48s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "# Shows time that regular python takes to merge file\n",
    "# Join all data together\n",
    "## here we are using a normal python way of merging the data \n",
    "\n",
    "files = glob.glob('figshare/*.csv') # load all the CSVs\n",
    "df = pd.concat((pd.read_csv(file, index_col=0) # combine them all\n",
    "                .assign(model=re.findall(r'/([^_]*)', file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"figshare/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "straight-jacob",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62513858</th>\n",
       "      <td>2014-12-27 12:00:00</td>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>6.689683e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62513859</th>\n",
       "      <td>2014-12-28 12:00:00</td>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>7.862555e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62513860</th>\n",
       "      <td>2014-12-29 12:00:00</td>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>1.000503e+01</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62513861</th>\n",
       "      <td>2014-12-30 12:00:00</td>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>8.541592e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62513862</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>-30.157068</td>\n",
       "      <td>-29.214660</td>\n",
       "      <td>153.1250</td>\n",
       "      <td>154.3750</td>\n",
       "      <td>6.811749e+01</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62513863 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0         1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1         1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2         1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3         1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4         1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "...                       ...        ...        ...       ...       ...   \n",
       "62513858  2014-12-27 12:00:00 -30.157068 -29.214660  153.1250  154.3750   \n",
       "62513859  2014-12-28 12:00:00 -30.157068 -29.214660  153.1250  154.3750   \n",
       "62513860  2014-12-29 12:00:00 -30.157068 -29.214660  153.1250  154.3750   \n",
       "62513861  2014-12-30 12:00:00 -30.157068 -29.214660  153.1250  154.3750   \n",
       "62513862  2014-12-31 12:00:00 -30.157068 -29.214660  153.1250  154.3750   \n",
       "\n",
       "          rain (mm/day)            model  \n",
       "0          4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1          4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2          4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3          4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4          4.270161e-13  MPI-ESM-1-2-HAM  \n",
       "...                 ...              ...  \n",
       "62513858   6.689683e+00      SAM0-UNICON  \n",
       "62513859   7.862555e+00      SAM0-UNICON  \n",
       "62513860   1.000503e+01      SAM0-UNICON  \n",
       "62513861   8.541592e+00      SAM0-UNICON  \n",
       "62513862   6.811749e+01      SAM0-UNICON  \n",
       "\n",
       "[62513863 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.read_csv(\"./figshare/combined_data.csv\")\n",
    "df_combined # combined dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-assumption",
   "metadata": {},
   "source": [
    "### Summary of Performance on Different Machines\n",
    "\n",
    "Everyone in the team tried to run the combined files section and we have recorded our time comsuption and detailed `RAM`, `processor`, and `IF SSD` to check if they are relevant.\n",
    "\n",
    "Below is the summarized table:\n",
    "\n",
    "| Team Member      |  Operating System |RAM |Processor |Is SSD|Time used to combine csv files |Time used to load combined csv to memory |\n",
    "| ----------- | ----------- |----------- |----------- |----------- |----------- |----------- |\n",
    "| Rachel      |  MacOS Catalina Version 10.15.6 |16GB of 3733MHz       |2 GHz Quad-Core Intel Core i5       |Yes    |peak memory: 404.39 MiB, increment: 0.05 MiB, CPU times: user 5min 29s, sys: 19.5 s, total: 5min 48s Wall time: 6min       |peak memory: 7112.71 MiB, increment: 3458.57 MiB CPU times: user 58.7 s, sys: 15.7 s, total: 1min 14s Wall time: 1min 23s       |\n",
    "| Daniel   |  Windows 10 Education |8 GB        |1.6 GHz Intel Core i5        |Yes        |peak memory: 2596.04 MiB, increment: 0.89 MiB,Wall time: 28min 59s        |peak memory: 3737.39 MiB, increment: 0.82 MiB,Wall time: 6min 13s        |\n",
    "| Santiago   |  Windows 10 Education |8 GB 2400 MHz   |2 GHz Intel Core i7-8550U        |Yes        |peak memory: 297.27 MiB, increment: 0.06 MiB, Wall time: 24min 47s       |peak memory: 2803.73 MiB, increment: 1.09 MiB,Wall time: 4min 19s      |\n",
    "| Rui   |  MacOS Catalina Version 10.15.7 |16 GB 2133 MHz |2.9 GHz Quad-Core Intel Core i7        |Yes        |peak memory: 356.82 MiB, increment: 0.31 MiB, CPU times: user 6min 43s, sys: 19.4 s, total: 7min 3s Wall time: 7min 13s        |peak memory: 2983.91 MiB, increment: 0.19 MiB,CPU times: user 1min, sys: 15 s, total: 1min 15s, Wall time: 1min 21s        |\n",
    "\n",
    "As we can see from the performance data we collected, the `Operating system`,`RAM`, `Proccessor` and `Hard Drive Type` could influence the data combining efficiency. As for this particular action, we realize the `oprating system` and `RAM` might be highly correlated to the time spent. \n",
    "\n"
    "We noticed that with Mac operating systems the code ran efficiently from top to bottom, whereas Windows operating systems ran slower. This may also be due to the GB of RAM (random-access memory) that the Mac users of our team had vs. the Windows users (Mac users had 16GB of RAM, double what the Windows users had). RAM seems to play an important role here when accessing large data because RAM is the area to store and access working data on a short-term basis, therefore, the larger our data, the more RAM we will need for peak performance and speed. Overall, from the table above we can see that Mac users with 16GB ran the code much faster with a lower peak memory on average than the Windows users with 8GB. This is particularily evident in combining CSV files where Windows users took almost 30min to run while Mac users took around 7min. We also noticed that Windows users ran into an `index error` when running the code to combine the CSVs. We combatted this issue by adding a `./` to the filename, but have removed it currently for Mac systems to process.
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "meaningful-homeless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MPI-ESM-1-2-HAM', 'AWI-ESM-1-1-LR', 'NorESM2-LM', 'ACCESS-CM2',\n",
       "       'FGOALS-f3-L', 'CMCC-CM2-HR4', 'MRI-ESM2-0', 'GFDL-CM4',\n",
       "       'BCC-CSM2-MR', 'EC-Earth3-Veg-LR', 'CMCC-ESM2', 'NESM3',\n",
       "       'MPI-ESM1-2-LR', 'ACCESS-ESM1-5', 'FGOALS-g3', 'INM-CM4-8',\n",
       "       'MPI-ESM1-2-HR', 'TaiESM1', 'NorESM2-MM', 'CMCC-CM2-SR5',\n",
       "       'observed', 'KIOST-ESM', 'INM-CM5-0', 'MIROC6', 'BCC-ESM1',\n",
       "       'GFDL-ESM4', 'CanESM5', 'SAM0-UNICON'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[\"model\"].unique() # print out the unique models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "close-arlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigshare/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figshare/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-department",
   "metadata": {},
   "source": [
    "We can see from our combined dataframe that we have 28 unique models to continue our analysis with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-confidence",
   "metadata": {},
   "source": [
    "### Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "existing-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "NESM3                966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 7112.71 MiB, increment: 3458.57 MiB\n",
      "CPU times: user 58.7 s, sys: 15.7 s, total: 1min 14s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "#simple pandas - This is how we do normally ,which means we are loading the entire data to the memory\n",
    "df = pd.read_csv(\"figshare/combined_data.csv\")\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "attached-vector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "crazy-clone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time              object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "model             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking datatypes for columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-recording",
   "metadata": {},
   "source": [
    "We can see that we have object and float64 type columns in our dataframe. This makes sense that `time` and `model` are object types and the rest such as latitude, longitude, and rain are float64 types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-county",
   "metadata": {},
   "source": [
    "### Investigate changing the `dtype` of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incredible-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory usage with the original float64 dtype: 1500.33 MB\n",
      "The memory usage after changing to float32 dtype: 750.17 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"The memory usage with the original float64 dtype: {df[['lat_min','lat_max','rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"The memory usage after changing to float32 dtype: {df[['lat_min','lat_max','rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-driving",
   "metadata": {},
   "source": [
    "### Observation1:\n",
    "> As we switch the data type from `float64` to `float32` the memory usage reduced by a half. This is because `float32` is stored as a 32-bit number, while `float64` is stored as twice as much memory as `float32`. If we have a large amount of data and we don't have a specific requirement on the precision or our original data is not accurate to a certain number of decimal places, `float32` is sufficient enough for us to process the data, which is not only faster but also resource-saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-cowboy",
   "metadata": {},
   "source": [
    "### Loading our data in chunks using Pandas and checking the value counts of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "precious-senior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "observed              46020\n",
      "dtype: int64\n",
      "peak memory: 2139.21 MiB, increment: 2031.93 MiB\n",
      "CPU times: user 54.7 s, sys: 10.6 s, total: 1min 5s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figshare/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-aruba",
   "metadata": {},
   "source": [
    "### Observation 2:\n",
    "> By loading in chunks, the value counts are exactly the same as other methods we tried. The peak memory for chunks is significantly lower than that without using chunking method. From our observation, we can conclude that for large-scaled data, if we choose to load in chunks, we can gain the competitive edge for lower memory usage and faster processing speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-tennessee",
   "metadata": {},
   "source": [
    "### Loading our data using Dask and checking the value counts of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "signed-blink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 5816.75 MiB, increment: 1481.29 MiB\n",
      "CPU times: user 1min 16s, sys: 19.6 s, total: 1min 36s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Dask\n",
    "df_dask = dd.read_csv('figshare/combined_data.csv')\n",
    "print(df_dask[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-booking",
   "metadata": {},
   "source": [
    "### Observation3:\n",
    "> So far, dask is the best pick for us to read large csv files to dataframe. Compared with loading the csv to pandas data frame, when we load the csv file to `dask`, the `peak memory`, `increment memory`, and `wall time` all reduced dramatically for the `value_count()` operation. This is likely because `dask` partitioned the dataframe based on row index and did the calculation in parallel to improve the efficiency. Thus, for large-scale data calculation, we could use dask instead of pandas to improve the code efficiency with minimum syntax change. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-piano",
   "metadata": {},
   "source": [
    "### Transfering the dataframe from Python to R using Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "conscious-compilation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3960.17 MiB, increment: 688.41 MiB\n",
      "CPU times: user 20.4 s, sys: 16.2 s, total: 36.6 s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "dataset = ds.dataset(\"figshare/combined_data.csv\", format=\"csv\")\n",
    "## this is of arrow table format\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "anonymous-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.34 s, sys: 14.1 s, total: 19.5 s\n",
      "Wall time: 9.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# writing in feather format\n",
    "feather.write_feather(table, 'figshare/combined_data.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-alliance",
   "metadata": {},
   "source": [
    "### Reason for choosing `feather`\n",
    "> Our team did a comprehensive comparision and research among the four data formats online and testing in practice, in the end `feather` is our best pick for this project scenario. Our reasoning is listed below:\n",
    "\n",
    "> - `Feather` enables us to store and read the data from raw arrow format without much serialization and deserialization which renders it faster (higher I/O speed) than Parquet, although Parquet can take less storage memory which is more suitable for long term data storage.\n",
    "    \n",
    "> - Feather is a columnar dataframe which can speed up the data analytics queries. \n",
    "    \n",
    "> - It has the unique competitive advantage for not taking too much memory without the need to unpacking the data before loading to RAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-interpretation",
   "metadata": {},
   "source": [
    "### Simple EDA in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adjusted-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(tidyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "needed-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: â€˜arrowâ€™\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from â€˜package:utilsâ€™:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: â€˜dplyrâ€™\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from â€˜package:statsâ€™:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from â€˜package:baseâ€™:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 28 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# â€¦ with 18 more rows\u001b[39m\n",
      "Time difference of 28.49756 secs\n",
      "CPU times: user 11.7 s, sys: 27 s, total: 38.7 s\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figshare/combined_data.feather\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model) # showing the different counts of the models \n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "express-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 92,040 x 2\u001b[39m\n",
      "   time                    n\n",
      "   \u001b[3m\u001b[90m<dttm>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m 1888-12-31 \u001b[90m16:00:00\u001b[39m    29\n",
      "\u001b[90m 2\u001b[39m 1889-01-01 \u001b[90m04:00:00\u001b[39m  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 3\u001b[39m 1889-01-01 \u001b[90m16:00:00\u001b[39m    29\n",
      "\u001b[90m 4\u001b[39m 1889-01-02 \u001b[90m04:00:00\u001b[39m  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 5\u001b[39m 1889-01-02 \u001b[90m16:00:00\u001b[39m    29\n",
      "\u001b[90m 6\u001b[39m 1889-01-03 \u001b[90m04:00:00\u001b[39m  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 7\u001b[39m 1889-01-03 \u001b[90m16:00:00\u001b[39m    29\n",
      "\u001b[90m 8\u001b[39m 1889-01-04 \u001b[90m04:00:00\u001b[39m  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 9\u001b[39m 1889-01-04 \u001b[90m16:00:00\u001b[39m    29\n",
      "\u001b[90m10\u001b[39m 1889-01-05 \u001b[90m04:00:00\u001b[39m  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m# â€¦ with 92,030 more rows\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "result <- r_table %>% count(time) # showing the different counts of the time\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-annual",
   "metadata": {},
   "source": [
    "### Observation From EDA: \n",
    "> The counts for models in R is the same as the counts for models we did previously in python, time to count is faster, which double confirmed the accuracy of EDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "catholic-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "r_table_d <- r_table %>% drop_na() # drop NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "secret-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "r_table_d <- r_table_d %>% rename(rain_mmperday = `rain (mm/day)`) # rename the column for rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lined-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# function to calculate the mode\n",
    "mode <- function(x) {\n",
    "  ux <- unique(x)\n",
    "  ux[which.max(tabulate(match(x, ux)))]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "surface-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Columns      Mean      Mode       Median\n",
      "1          lat_min -33.10482 -34.86911 -33.00000000\n",
      "2          lat_max -31.92201 -34.86911 -32.04188482\n",
      "3          lon_min 146.90590 144.37500 146.87500000\n",
      "4          lon_max 148.28781 144.37500 148.12500000\n",
      "5 rain (mm/perday)   1.90117   0.00000   0.06154947\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "Columns <- c(\"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/perday)\")\n",
    "Mean <- c(mean(r_table_d$lat_min), mean(r_table_d$lat_max), mean(r_table_d$lon_min), mean(r_table_d$lon_max), mean(r_table_d$rain_mmperday))\n",
    "Mode <- c(mode(r_table_d$lat_min), mode(r_table_d$lat_max), mode(r_table_d$lon_min), mode(r_table_d$lon_max), mode(r_table_d$rain_mmperday))\n",
    "Median <- c(median(r_table_d$lat_min), median(r_table_d$lat_max), median(r_table_d$lon_min), median(r_table_d$lon_max), median(r_table_d$rain_mmperday))\n",
    "\n",
    "result <- data.frame(Columns, Mean, Mode, Median)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-viewer",
   "metadata": {},
   "source": [
    "### Observation From Mean, Mode, Mean\n",
    "> The mean and median of location data(`lagtitude` and `longitude`) are very close, which means the data collected are mostly from the same area. The median and mean of `rain` is not quite close which indicates that they are not normally distributed and there might be outliers in the data for `rain`. The `mode` are close to the median which means if we randomly sample a value then we are likely sample a value close to the median. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-candidate",
   "metadata": {},
   "source": [
    "### Challenges and difficulties when dealing with large data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-cooperation",
   "metadata": {},
   "source": [
    "> Since we are running the code in the local machine, it took a long time to run. We combatted these problems by having to restart from scratch if there's anything we want to modify from the start which is frustrating. \n",
    "\n",
    "> Everytime we reran the notebook we had to delete the downloaded files and redownload it again, which is quite challenging for large-scale data processing.\n",
    "\n",
    "> As we only have one single machine, our EDA was very simple, we can hardly visualize our data or calculate correlation matrices. We were unable to do deep EDA like plots (histograms, correlation matrices, etc.) to show relationships between features because the data was so large and taking a sample of the data would not be ideal. To combat this we focused on simpler EDA methods such as analyzing mean/median/mode and count values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-terror",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
